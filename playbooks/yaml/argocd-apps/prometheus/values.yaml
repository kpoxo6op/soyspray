# playbooks/yaml/argocd-apps/prometheus/values.yaml

namespaceOverride: monitoring

prometheus:
  service:
    type: LoadBalancer
    loadBalancerIP: 192.168.1.125
  prometheusSpec:
    replicas: 1
    retention: 15d
    walCompression: true
    # Global scrape configurations
    scrapeInterval: 60s
    scrapeTimeout: 30s
    evaluationInterval: 60s
    resources:
      requests:
        cpu: 150m
        memory: 384Mi
      limits:
        cpu: 500m
        memory: 1Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    additionalScrapeConfigs:
      - job_name: pihole-exporter
        static_configs:
          - targets: ["pihole-exporter.pihole.svc.cluster.local:9617"]
        metrics_path: /metrics
        scrape_interval: 60s
        scrape_timeout: 30s
    ruleSelector:
      matchLabels:
        release: prometheus

  additionalServiceMonitors:
    - name: cert-manager
      selector:
        matchLabels:
          app.kubernetes.io/name: cert-manager
      namespaceSelector:
        matchNames:
          - cert-manager
      endpoints:
        - port: tcp-prometheus-servicemonitor
          interval: 60s

    - name: argocd
      selector:
        matchLabels:
          app.kubernetes.io/name: argocd-metrics
      namespaceSelector:
        matchNames:
          - argocd
      endpoints:
        - port: metrics
          interval: 60s

grafana:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 75m
      memory: 96Mi
    limits:
      cpu: 150m
      memory: 192Mi
  grafana.ini:
    auth.anonymous:
      enabled: true
      org_name: "Main Org."
      org_role: "Viewer"
    date_formats:
      default_timezone: "Pacific/Auckland"
  defaultDashboardsTimezone: "Pacific/Auckland"
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      provider:
        name: 'default'
        orgId: 1
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      provider:
        name: 'default'
        orgId: 1

  service:
    type: ClusterIP
  adminPassword: "admin"
  image:
    tag: 11.3.0

  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      kubernetes.io/ingress.class: nginx
    labels: {}
    hosts:
      - grafana.soyspray.vip
    path: /
    tls:
      - secretName: prod-cert-tls
        hosts:
          - grafana.soyspray.vip

alertmanager:
  alertmanagerSpec:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
    volumes:
      - name: telegram-secret
        secret:
          secretName: alertmanager-telegram-secret
    volumeMounts:
      - name: telegram-secret
        mountPath: /etc/alertmanager/telegram
        readOnly: true
  service:
    type: LoadBalancer
    loadBalancerIP: 192.168.1.126
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ["alertname"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: "telegram"
      routes:
        - matchers:
            - severity=~"warning|critical"
          receiver: "telegram"
    receivers:
      - name: "telegram"
        telegram_configs:
          - api_url: "https://api.telegram.org"
            bot_token_file: /etc/alertmanager/telegram/TELEGRAM_BOT_TOKEN
            chat_id: 336642153

prometheus-node-exporter:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

kube-state-metrics:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

# Disabled until proper etcd certificates are configured for Prometheus to
# access etcd metrics
kubeEtcd:
  enabled: false

# https://github.com/onzack/grafana-dashboards/tree/main?tab=readme-ov-file#troubleshooting
ruleSelectorNilUsesHelmValues: false

prometheusOperator:
  admissionWebhooks:
    enabled: true
    patch:
      enabled: true
      hookDeletePolicy: before-hook-creation,hook-succeeded
  resources:
    requests:
      cpu: 75m
      memory: 96Mi
    limits:
      cpu: 150m
      memory: 192Mi
  service:
    port: 8443
    targetPort: 8443
  containerArgs:
    - --web.listen-address=:8443
    - --web.enable-tls=false
