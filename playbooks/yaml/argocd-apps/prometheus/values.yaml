# playbooks/yaml/argocd-apps/prometheus/values.yaml
# kubectl exec -n monitoring alertmanager-kube-prometheus-stack-alertmanager-0 -c alertmanager -- cat /etc/alertmanager/config/soy-telegram.tmpl
namespaceOverride: monitoring

prometheus:
  service:
    type: LoadBalancer
    loadBalancerIP: 192.168.1.34
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - prometheus.soyspray.vip
    paths:
      - /
    pathType: Prefix
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.soyspray.vip
  prometheusSpec:
    image:
      tag: v3.6.0
    replicas: 1
    retention: 15d
    walCompression: true
    podMonitorSelectorNilUsesHelmValues: false
    scrapeInterval: 60s
    scrapeTimeout: 30s
    evaluationInterval: 60s
    externalLabels:
      cluster: soyspray
    resources:
      requests:
        cpu: 150m
        memory: 384Mi
      limits:
        cpu: 500m
        memory: 1Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    additionalScrapeConfigs: []

  additionalServiceMonitors:
    - name: cert-manager
      selector:
        matchLabels:
          app.kubernetes.io/name: cert-manager
      namespaceSelector:
        matchNames:
          - cert-manager
      endpoints:
        - port: tcp-prometheus-servicemonitor
          interval: 60s

    - name: argocd
      selector:
        matchLabels:
          app.kubernetes.io/name: argocd-metrics
      namespaceSelector:
        matchNames:
          - argocd
      endpoints:
        - port: metrics
          interval: 60s

grafana:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 75m
      memory: 96Mi
    limits:
      cpu: 150m
      memory: 192Mi
  grafana.ini:
    auth.anonymous:
      enabled: true
      org_name: "Main Org."
      org_role: "Viewer"
    date_formats:
      default_timezone: "Pacific/Auckland"
  defaultDashboardsTimezone: "Pacific/Auckland"
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      provider:
        name: 'default'
        orgId: 1
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      provider:
        name: 'default'
        orgId: 1
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki.monitoring.svc.cluster.local:3100
          isDefault: false
          editable: true
          jsonData:
            httpMethod: GET
            timeout: 60

  service:
    type: ClusterIP
  adminPassword: "admin"
  image:
    tag: 11.3.0

  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      kubernetes.io/ingress.class: nginx
    labels: {}
    hosts:
      - grafana.soyspray.vip
    path: /
    tls:
      - secretName: prod-cert-tls
        hosts:
          - grafana.soyspray.vip

alertmanager:
  alertmanagerSpec:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
    volumes:
      - name: telegram-secret
        secret:
          secretName: alertmanager-telegram-secret
    volumeMounts:
      - name: telegram-secret
        mountPath: /etc/alertmanager/telegram
        readOnly: true
  service:
    type: LoadBalancer
    loadBalancerIP: 192.168.1.35
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ["alertname","namespace","severity"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: "telegram"
      routes:
        - matchers:
            - severity=~"warning|critical"
          receiver: "telegram"
    receivers:
      - name: "telegram"
        telegram_configs:
          - api_url: "https://api.telegram.org"
            bot_token_file: /etc/alertmanager/telegram/PROMETHEUS_TELEGRAM_BOT_TOKEN
            chat_id: 336642153
            parse_mode: HTML
            message: '{{ template "soy.telegram" . }}'
    templates:
      - '/etc/alertmanager/config/*.tmpl'
  templateFiles:
    soy-telegram.tmpl: |
      {{- define "soy.telegram" -}}
      {{- if eq .Status "firing" -}}ðŸ”¥ FIRING{{- else -}}âœ… RESOLVED{{- end }} [{{ .CommonLabels.severity }}] {{ .CommonLabels.alertname }} ({{ len .Alerts }})

      {{- range .Alerts }}
      - Alert
        ns={{ .Labels.namespace }}{{ if .Labels.deployment }} deploy={{ .Labels.deployment }}{{ end }}{{ if .Labels.pod }} pod={{ .Labels.pod }}{{ end }}{{ if .Labels.container }} container={{ .Labels.container }}{{ end }}{{ if .Labels.node }} node={{ .Labels.node }}{{ end }}{{ if .Labels.reason }} reason={{ .Labels.reason }}{{ end }}
        {{- $text := .Annotations.summary -}}
        {{- if eq $text "" -}}{{- $text = .Annotations.description -}}{{- end -}}
        {{ $text }}
        {{- if .GeneratorURL }}
        Source: <a href="{{ .GeneratorURL }}">open</a>
        {{- end }}

      {{- end }}
      {{- if .CommonAnnotations.runbook_url }}
      Runbook: <a href="{{ .CommonAnnotations.runbook_url }}">open</a>
      {{- end }}
      {{- end }}

prometheus-node-exporter:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

kube-state-metrics:
  namespaceOverride: monitoring
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

kubeEtcd:
  enabled: false

prometheusOperator:
  admissionWebhooks:
    enabled: true
    patch:
      enabled: true
      hookDeletePolicy: before-hook-creation,hook-succeeded
  tls:
    enabled: false
  resources:
    requests:
      cpu: 75m
      memory: 96Mi
    limits:
      cpu: 150m
      memory: 192Mi
  service:
    port: 8443
    targetPort: 8443
  containerArgs:
    - --web.listen-address=:8443
    - --web.enable-tls=false
