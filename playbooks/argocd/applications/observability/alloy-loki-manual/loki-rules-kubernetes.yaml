apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-rules-kubernetes
  namespace: monitoring
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/component: ruler
    app.kubernetes.io/part-of: observability
    loki.grafana.com/rule: "1"
data:
  kubernetes-critical.yaml: |
    groups:
      - name: kubernetes-critical
        rules:
          - alert: KubernetesPodCrashLoopingLogs
            expr: |
              sum by (namespace, pod) (
                label_replace(
                  count_over_time({cluster="soyspray", job="kubernetes-events"} |= "Back-off restarting failed container"[5m]),
                  "pod", "$1", "kubernetes_event_involved_object_name", "(.*)"
                )
              ) > 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: |
                Loki detected repeated CrashLoopBackOff events for pod {{ $labels.namespace }}/{{ $labels.pod }} in the last 10 minutes.

  application-errors.yaml: |
    groups:
      - name: application-errors
        rules:
          - alert: ApplicationErrorBurst
            expr: |
              sum by (namespace, pod) (
                count_over_time(
                  {cluster="soyspray", job="kubernetes-pods", pod!~"loki.*"}
                  !~ "(?i)tiers.crd.projectcalico.org is forbidden"
                  !~ "(?i)example-benign-error"
                  !~ "(?i)another-example-error"
                  |~ "(?i)\\b(error|fatal)\\b"
                  [5m]
                )
              ) > 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Error burst in {{ $labels.namespace }}/{{ $labels.pod }}"
              description: |
                More than 50 log lines containing 'error' or 'fatal' were observed in 5m for pod {{ $labels.namespace }}/{{ $labels.pod }}. Investigate recent deployments, crashes, or upstream dependencies.

  storage-mount-failures.yaml: |
    groups:
      - name: storage-mount-failures
        rules:
          - alert: VolumeMountAttachFailures
            expr: |
              sum by (namespace, pod) (
                label_replace(
                  count_over_time(
                    {cluster="soyspray", job="kubernetes-events", type="Warning"}
                    |~ "(?i)FailedMount|FailedAttachVolume|VolumeAttachmentFailed|Unable to attach or mount volumes|MountVolume\\.SetUp failed"
                    [10m]
                  ),
                  "pod", "$1", "kubernetes_event_involved_object_name", "(.*)"
                )
              ) > 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: "Volume attach/mount failure for {{ $labels.namespace }}/{{ $labels.pod }}"
              description: |
                Kubernetes reported volume attach/mount warnings (FailedMount/FailedAttachVolume) for {{ $labels.namespace }}/{{ $labels.pod }} within the last 10 minutes. Check Longhorn volume status, node health, and kubelet logs.

